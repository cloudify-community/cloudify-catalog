tosca_definitions_version: cloudify_dsl_1_4

description: Cloudify Kafka blueprint. Deploys Kafka service with Ansible Cloudify plugin.

imports:
  - https://raw.githubusercontent.com/cloudify-community/eaas-example/master/utils/custom_types.yaml
  - cloudify/types/types.yaml
  - plugin:cloudify-fabric-plugin
  - plugin:cloudify-ansible-plugin

inputs:

  infra_name:
    display_label: Name of the provider to deploy resources
    description: >
      Name of infrastructure blueprint to deploy.
    type: string
    constraints:
      - valid_values:
          - azure
          - aws

  # we using the virtual machine image that is under maintenance
  infra_archive:
    display_label: URL for infra zip archive
    description: >
      URL of infra zip file.
    type: string
    default: https://github.com/cloudify-community/cloudify-catalog/raw/6.4.0-build/docker/vm/vm.zip

  infra_exists:
    type: string
    display_label: Set if infra exists or not 
    description: >
      Whether a getting started infrastructure blueprint has already been uploaded to the manager or not.
    default: false

  infra_deployment_id:
    type: string
    display_label: The infrastructure deployment id
    description: The blueprint name, the deployment name.
    default: { concat: [ 'infra-', { get_input: infra_name } ] }

node_templates:

  #we creating the prefix for the deployment resources i.e. infrastructure
  infra_prefix:
    type: eaas.nodes.UniquePrefixGenerator
    properties:
      predefined_value: ""

  #generating the password for the kakfa user
  master_password:
    type: cloudify.nodes.PasswordSecret
    properties:
      length: 12
      uppercase: 0
      lowercase: 7
      digits: 2
      symbols: 3
      use_secret_if_exists: false

  # deployment of the infrastructure, we pass the URL as a blueprint archive
  # the node will automatically fetch the file from the URL host and unzip it
  # we point the node to use particular infrastructure cloud provider within 
  # main_file_name param by setting infra_name input as the file name value
  infrastructure:
    type: cloudify.nodes.Component
    properties:
      resource_config:
        blueprint:
          blueprint_archive: { get_input: infra_archive }
          main_file_name: { concat: [ { get_input: infra_name }, '.yaml' ] }
          external_resource: { get_input: infra_exists }
        deployment:
          id: { concat: [ get_attribute: [ infra_prefix, value ], "-", { get_input: infra_deployment_id } ] }
      secrets:
        aws_access_key_id: { get_secret: aws_access_key_id }
        aws_secret_access_key: { get_secret: aws_secret_access_key }
        subscription_id: { get_secret: azure_subscription_id }
        tenant_id: { get_secret: azure_tenant_id }
        client_id: { get_secret: azure_client_id }
        client_secret: { get_secret: azure_client_secret }
    interfaces:
      cloudify.interfaces.lifecycle:
        configure:
          implementation: cfy_extensions.cloudify_types.component.create
          inputs: 
            secrets: 
              dummy: dummy
    relationships:
      - type: cloudify.relationships.depends_on
        target: infra_prefix
  
  # # we setting the kafka port to be open
  secrurity_group_rules:
    type: cloudify.nodes.Component
    properties:
      resource_config:
        blueprint:
          blueprint_archive: "security_group.zip"
          main_file_name: { concat: [ { get_input: infra_name }, '.yaml' ] }
          external_resource: false
        deployment:
          id: { concat: [ get_attribute: [ infra_prefix, value ], "-security-group" ] }
          inputs:
            rg_id: { get_capability: [ { concat: [ get_attribute: [ infra_prefix, value ], "-", { get_input: infra_deployment_id } ] } , rg_id ] }
            vpc_id: { get_capability: [ { concat: [ get_attribute: [ infra_prefix, value ], "-", { get_input: infra_deployment_id } ] } , vpc_id ] }
            security_group_id: { get_capability: [ { concat: [ get_attribute: [ infra_prefix, value ], "-", { get_input: infra_deployment_id } ] } , security_group_id ] }
    relationships:
      - type: cloudify.relationships.depends_on
        target: infrastructure

  # deployment of the service is made by ansible plugin which run the code 
  # inside the 'install_kafka.yaml' playbook
  kafka:
    type: cloudify.nodes.ansible.Playbook
    interfaces:
      cloudify.interfaces.lifecycle:
        poststart: {}
    relationships:
      - type: cloudify.ansible.relationships.run_on_host
        target: infrastructure
        source_interfaces:
          cloudify.interfaces.relationship_lifecycle:
            establish:
              inputs:
                playbook_path: playbooks/install_kafka.yaml
                sources:
                  instances:
                    hosts:
                      instance:
                        ansible_host: { get_capability: [ { concat: [ get_attribute: [ infra_prefix, value ], "-", { get_input: infra_deployment_id } ] } , endpoint ] }
                        ansible_user:  { get_capability: [ { concat: [ get_attribute: [ infra_prefix, value ], "-", { get_input: infra_deployment_id } ] }, user ] }
                        ansible_ssh_private_key_file:  { get_capability: [ { concat: [ get_attribute: [ infra_prefix, value ], "-", { get_input: infra_deployment_id } ] }, key_content ] }
                        ansible_become: true
                        ansible_ssh_common_args: -o StrictHostKeyChecking=no
                run_data:
                   master_password: { get_attribute: [ master_password, password ] }

outputs:

  kafka_ip:
    description: Kafka Endpoint
    value: { get_capability: [ { concat: [ get_attribute: [ infra_prefix, value ], "-", { get_input: infra_deployment_id } ] }, endpoint] }

  kafka_port: 
    description: Kafka port
    value: 9092
  
  kafka_user_pwd:
    description: Kafka user pwd
    value: { get_attribute: [ master_password, password ] }
  
